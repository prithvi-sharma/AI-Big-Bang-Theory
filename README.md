# AI-Big-Bang-Theory

<img src="https://78.media.tumblr.com/0bf1a3a2bfe9978bf55d0af993bf4131/tumblr_p6ghogSTk91wfkg1xo1_500.gif" width="100%">

The project had an initial idea of building a text prediction model like normal mobile keyboards. Then I fount out about GPT-2 and tried it instead of LSTM for the first time. I made this model with using '355M' and got the transcipts from https://bigbangtrans.wordpress.com
<hr>

# OpenAI

</br></br>
<p align="center">
  <img src="https://miro.medium.com/max/700/1*UZTblFMn8oWfmqBMQhUOdA.jpeg" width="80%">
</p>
</br></br>
GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.
</br></br>
GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In addition, GPT-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute
<hr>

# GPT-2


Link :- <a href="https://github.com/openai/gpt-2">GPT-2</a>
<hr>

# Script
<object data="https://github.com/prithvi-sharma/AI-Big-Bang-Theory/blob/master/Script/AI%20Big%20Bang%20Theory.pdf" type="application/pdf" width="700px" height="700px">
    <embed src="https://github.com/prithvi-sharma/AI-Big-Bang-Theory/blob/master/Script/AI%20Big%20Bang%20Theory.pdf">
        <p>PDF Script: - <a href="https://github.com/prithvi-sharma/AI-Big-Bang-Theory/blob/master/Script/AI%20Big%20Bang%20Theory.pdf">AI BIg Bang Theory Script</a>.</p>
    </embed>
</object>
<hr>

## Thanks for Reading
The script is in the script folder and some interesting output and conversations are in the other folder. You can email me of whatever you wanna try and I will get back to you ASAP ðŸ˜‰.

<!--<strong>Also you can check out same project on my portfolio here -> </strong><a href="https://sites.google.com/view/prithvieportfolio/project-page/ai-big-bang-theory" alt="AI Big Bang Theory"><img src="https://image.flaticon.com/icons/svg/975/975645.svg" width=19px></a>-->

<p align="center">
<img src="https://media1.tenor.com/images/475f0154c049721e4b4ef126749e7aac/tenor.gif" width="80%"></p>

<strong>Cheers!!!!!!!!!</strong>
